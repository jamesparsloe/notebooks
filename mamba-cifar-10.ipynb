{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in ./env/lib/python3.11/site-packages (0.17.0)\n",
      "Requirement already satisfied: wandb in ./env/lib/python3.11/site-packages (0.16.3)\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.2.0 in ./env/lib/python3.11/site-packages (from torchvision) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./env/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (4.9.0)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in ./env/lib/python3.11/site-packages (from torch==2.2.0->torchvision) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./env/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->torchvision) (12.3.101)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in ./env/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in ./env/lib/python3.11/site-packages (from wandb) (3.1.41)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./env/lib/python3.11/site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in ./env/lib/python3.11/site-packages (from wandb) (1.40.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./env/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in ./env/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in ./env/lib/python3.11/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.11/site-packages (from wandb) (65.5.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in ./env/lib/python3.11/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in ./env/lib/python3.11/site-packages (from wandb) (4.25.2)\n",
      "Requirement already satisfied: six>=1.4.0 in ./env/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./env/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.11/site-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests->torchvision) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.11/site-packages (from requests->torchvision) (2024.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./env/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.11/site-packages (from jinja2->torch==2.2.0->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./env/lib/python3.11/site-packages (from sympy->torch==2.2.0->torchvision) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/github/jamesparsloe/notebooks/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Compose, PILToTensor\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaConfig, MambaLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, n_classes = \"cifar-10\", 10\n",
    "# dataset, n_classes = \"cifar-100\", 100\n",
    "\n",
    "DatasetCls = CIFAR10 if dataset == \"cifar-10\" else CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "root = os.path.expanduser(\"~/.cache/torchvision\")\n",
    "os.makedirs(root, exist_ok=True)\n",
    "\n",
    "train_ds = DatasetCls(root, train=True, download=True)\n",
    "val_ds = DatasetCls(root, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, cls = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDiNHsJ7GBpGkcyuTncxP5VcuL77Kd1xNIN6jgueTV/Vp47VA4jQsW2qBwM1a0rwjH4guoJrlzFJOhaGF+gUYCn8TmvKcm3d9T00klZdDJjv9k5t51nSUjdtbI7defan3+pXlrHsjE0rYyNr9vxrtb/AOHEsiyJMj3FyUxEWfbg/wBa5Wa1kspWtbgbZYTsYHsRxQmtGN8uyZLaQ2tzqMX9oiQ2iS732KC3XoP89K7a+1ZreS71HT7WJUCxJA7Dd5arwRj/ADyawLjR5DdPNYFmjDElAD61UN5qkKypvmUMR8u0jkcVjTrKashuKubdp4tljvlu9UeSeJPmUhgu1/b0HasG6ln1fWnkhZDb3M2UV+4J5qlOiiIGeLeM/MhBIz649afFqLRWuqtZWUs1xBCixkqQihjg49wKc1KasZyjbY//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAJgklEQVR4ARVW2a7cWBU9o31sl13TreFOyU1yc0NoddLQAbXC2OoXeEG88XfwAwihFkJC4qGFBDzQrSCahCZkvmPdmlxl+9g+IztHKpVKtnbts/baay38gx//JM9XIXGDwN8YxqNBstPrBJSzMEKUrda5Mr7f6xKr27ZtmkZEwiIr67Lby5C3qlUUcUpp2ukkScK5qFvlMUGEwSPjMXv67Gm+WAwEwkOxY1McjSu3Kq33OJCNknWrrVtQLJg3xlHCwjCUTWWcws2QUAR/GzFRtmplTRwnmHBMOSJENtpoTVnIIoZRiG4OxdGkOx4NIngJ4xp61a3HOIgiZLx3bXcQG+0DHlmLaBC2CgrgOAhZEokgNLgi3hmEKUadJC4rCeUJRsV2wwQ2acpO9vvDiHLXlCtlHamlIQHKeh0WhPmmYAwN0rjYVqqp6kZ7hAEOrWpiGQ9DazWjuG11wAPiTFuukfUhRca5TdWyPlwiDLtJNMq4ddAfIE/hjq3TDI53tq09JdfXudW2kFJa1Yky1FqKHMGehqKumphnzPumUbU2Dvm8bHKpS2kaTdioJ1JOhaCE+iiKtLEOYe8VDMgq7byGL8+CQlXWUmkdPC8qfb6qOHFZifXVot7IGzvH4/EBTjftelmW1aZoFpv6zenGUsb2RkkWmE4cYK8R8hjwriWBkafdJBHbzaKbZUWj354vypYGDu3HjPH6zTJvPeXYd7P08bcfbS+tl767w1vJypKEnB9O0/F4Mts2bJBGTOUhZ3EYt7XWzvR6fQ9XsETrJu50Lubty7ebeWGkQTcj+ssffXSw2/ntV6/+/uIKuMSIL/K5LNs05chiIXggaIy5sebG4V66Kth4MKxXDcGslLpWhmEqtSUI1Vr1+pmy/tXZxWprASVKSSbsmBVi1d7NppcDMsuvW6mePH9OjNNJhroToH+3G6fON0p7tT0aJay/M+p3IkJ4vl3rqiQWZuA8Z52O0Ej859Xzqq2ECEXAoiTuU/PVi5lRrO1OR32BUaZNI1VdSa+MwVohjDiMnsCiMtO2HhpDsBqcI4RCwWOUwG84Grkw6i6uCrlY3x6ItkEiie/d2SdtYyjfbteMbtIgGfbv3Ll74/W7f3zz/DxgrfelMYwwoCt3cBBsFGHAa6xrhExVbZUmhohSFltZ7B/C7hY3d/CdPS4bvH/yMPDNeqOj3hAt6eF0N6+q29+6m/XjrH9/PS/Wmw0PEuJD7SzUttrAosEsmcXWW1hWH4mok8YX8/r12ZxxH8wumtn87ph/9tO7L89X6f5oZzi9ns96vYQ42Ch6PT9nIp/nl+eXJedxL3N1DfUIJtg5S953T6xHrNfrGGbKsvHaborN23ezsiwjQS5fbyci2N+/2du7xQuHBD94+H1xdR6ZuUVNVTW78QiWHiedg2Qv7U2L5dX1bKkxb1SLiE9CoeoSsGJFvmSq4KB/FDFKZbnpp0kvEfV6O94b7j/4yb/P1PMX6vHuIM/V5M5DgqRq5z3vttfLSOndwSC3IX/Qr/PLv/7x87PTOQ1gorj2SMM49XsVQbYuQV4IMhbTtUbbrfet2u0m3/v004N7n/zuN7+eJh2q6vNXL6e3vy2Gx4kv5Oo6cn1Vy0Uhe6Nbw+lRXWYkQzZoACKtFYZiAL1hDHsYiAa8GEG+1tihwTCexua7j07uP/5kfV2GZnP74MBhNx2PTGNkDipidM0s6rw8P/v6318+/kQNp8NtcQ0s3DlKHECvrGnVZp63RcxAXOrWBUkHFIASdTzti4gc3Tx8+MNPd+89+Offf3PjsD/94MNgdIfFXdmU9baYXZyuZ2dWyygVOzv89OLJZHffyNLXLa7W1tcgOFHIgynfhhiqsnUhbYOjOKLEj4fx6WV+57s/O/jwZwj1dVF10+7o5KOKDZ4++UdbV9ttvjh/R60Sgu3f2n9wcmxowmmPB5o1jXx7Dh0bgkpK42Ey2Ruytm7ikGFBOQH9NFGH/uJXv3j888+yncns1X8oMXmxmb/570Vhv/j97zsRb9pyOgGJS16fnSpiBntHJx9+jGy4ys9gXda1wZ41tSuB+GVzv4eY8wo5i40zXmPsRZh99PHHIIfP/vlkffESbLhYr05fPCt9xG3TYTQTyajfvZxdgSPKojx9/Q6hp2VZvPfUcLw0WRSJOI0iFhZyC1UZQs6BuPDYGquQmXT7f/r8D4PJ0/HuoZIbzsNOkjECKPDpeFgX64iGy/lCK5uKSJXl/558efnN89bU4PwWXjtIUKJI2Ahn+ii6/8Et5hwOGBXMoff+lDilF4urcn4V6a1DdNAf9vZGxrbnF1ceeeAaUIhinogYrIvCB8ioNsThrVyrsE732irKCwfeSobZ7Z3xkBAcijDyyMSRGA/HXrfDNOiGRm1mqlhIWYTZgCTDew8eORYpD6XAUqSzKKBMcGaMeX42//LZxdcvL1dmK3qMB0FZmqr2STqspSUBI6ptnQ8cDcERKHWxiJJ0FMTdyXinWM+l0qPDY+nCD773g/sfPSJMVGUrZQ1ig5G7PL949/qqlHXUgUw1xg3Hl0n/eudE3DroHbx4dsUmI6KXy9q6qgIILPh8lg0DzoGQEWdIsS//9rfb92ZnZ1dw2TiEjBVGUVKVNRxjIACEj79zItLMUAObUZ82pBDjOP3OyQfj3uSry9fsxmHQxeLFqZzNwSbDTodVcmNdCcliNV8WJSSDDfWbtNOfXa3OqsZ5PBkNsdPrfB0mYa+bBpS0yiLGq5aoksMqHx9O96bD07PZci5Z1uf1XPbHFCXxYtY2SrEgU0BdDTGl3dTrJAob2dTNQmlIQBD5aLmVWRZlWbcGLVquITOC0mDjwfRCgYKAHh0f1dL/5S/P/vX8mjHBRBYMOmA9LY8cmBWyBOZtOSSiPIgZf+/GceuBXwoCJWiXV41tEGccQSxbr2ulIacyMEIWSGRmi2JdmqLa/PmLb2YSASU4otBEw6P3It7tunJbl9tZKa1ubBoMBefgroyRgCAeUvCRuAO+CNprgohlvXi1KgrvssFQGvW/N8tvvj6dDLLJQYyI2+mm7OwtanORjoyIdLeDBgMGyTLP5XoZrJeIOuq8txD4wKRA5gmGKFVb4g3iThu5ssBExvNSwhRW2/rNi2W+rCCkTbvT+zf3tzVilu/o4FHrWmIWoot7I9EHhZEuX0X5gtYVg0aBXs44mEMAADNaNK4uG+5VSlJHtmAqYeIFD3uBuo16Hz5M7j14eHR8/P1P5NlF+X8Wt9uThtmfRQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# NOTE: use PILToTensor to keep things as uint8, ToTensor will convert to float32 in [0.0, 1.0]\n",
    "train_ds = DatasetCls(root, train=True, transform=PILToTensor(), download=True)\n",
    "val_ds = DatasetCls(root, train=False, transform=PILToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, cls = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 32, 32]), torch.uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape, img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_ds = DatasetCls(\n",
    "    root,\n",
    "    train=True,\n",
    "    transform=Compose([PILToTensor(), Rearrange(\"C H W -> H W C\")]),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "val_ds = DatasetCls(\n",
    "    root,\n",
    "    train=False,\n",
    "    transform=Compose([PILToTensor(), Rearrange(\"C H W -> H W C\")]),\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, cls = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "C, H, W = 3, 32, 32\n",
    "n_classes = 10\n",
    "rgb_offsets = 256 * torch.arange(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 59,  62,  63],\n",
       "         [ 43,  46,  45],\n",
       "         [ 50,  48,  43],\n",
       "         ...,\n",
       "         [158, 132, 108],\n",
       "         [152, 125, 102],\n",
       "         [148, 124, 103]],\n",
       "\n",
       "        [[ 16,  20,  20],\n",
       "         [  0,   0,   0],\n",
       "         [ 18,   8,   0],\n",
       "         ...,\n",
       "         [123,  88,  55],\n",
       "         [119,  83,  50],\n",
       "         [122,  87,  57]],\n",
       "\n",
       "        [[ 25,  24,  21],\n",
       "         [ 16,   7,   0],\n",
       "         [ 49,  27,   8],\n",
       "         ...,\n",
       "         [118,  84,  50],\n",
       "         [120,  84,  50],\n",
       "         [109,  73,  42]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[208, 170,  96],\n",
       "         [201, 153,  34],\n",
       "         [198, 161,  26],\n",
       "         ...,\n",
       "         [160, 133,  70],\n",
       "         [ 56,  31,   7],\n",
       "         [ 53,  34,  20]],\n",
       "\n",
       "        [[180, 139,  96],\n",
       "         [173, 123,  42],\n",
       "         [186, 144,  30],\n",
       "         ...,\n",
       "         [184, 148,  94],\n",
       "         [ 97,  62,  34],\n",
       "         [ 83,  53,  34]],\n",
       "\n",
       "        [[177, 144, 116],\n",
       "         [168, 129,  94],\n",
       "         [179, 142,  87],\n",
       "         ...,\n",
       "         [216, 184, 140],\n",
       "         [151, 118,  84],\n",
       "         [123,  92,  72]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 59, 318, 575],\n",
       "         [ 43, 302, 557],\n",
       "         [ 50, 304, 555],\n",
       "         ...,\n",
       "         [158, 388, 620],\n",
       "         [152, 381, 614],\n",
       "         [148, 380, 615]],\n",
       "\n",
       "        [[ 16, 276, 532],\n",
       "         [  0, 256, 512],\n",
       "         [ 18, 264, 512],\n",
       "         ...,\n",
       "         [123, 344, 567],\n",
       "         [119, 339, 562],\n",
       "         [122, 343, 569]],\n",
       "\n",
       "        [[ 25, 280, 533],\n",
       "         [ 16, 263, 512],\n",
       "         [ 49, 283, 520],\n",
       "         ...,\n",
       "         [118, 340, 562],\n",
       "         [120, 340, 562],\n",
       "         [109, 329, 554]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[208, 426, 608],\n",
       "         [201, 409, 546],\n",
       "         [198, 417, 538],\n",
       "         ...,\n",
       "         [160, 389, 582],\n",
       "         [ 56, 287, 519],\n",
       "         [ 53, 290, 532]],\n",
       "\n",
       "        [[180, 395, 608],\n",
       "         [173, 379, 554],\n",
       "         [186, 400, 542],\n",
       "         ...,\n",
       "         [184, 404, 606],\n",
       "         [ 97, 318, 546],\n",
       "         [ 83, 309, 546]],\n",
       "\n",
       "        [[177, 400, 628],\n",
       "         [168, 385, 606],\n",
       "         [179, 398, 599],\n",
       "         ...,\n",
       "         [216, 440, 652],\n",
       "         [151, 374, 596],\n",
       "         [123, 348, 584]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img + rgb_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(items):\n",
    "    batch = []\n",
    "    for img, cls in items:\n",
    "        token_ids = img + rgb_offsets # offset each of the rgb values - we'll share the Embedding\n",
    "        token_ids = rearrange(token_ids, \"H W C -> (H W C)\")\n",
    "        token_ids = token_ids + n_classes # offset by the number of classes\n",
    "        token_ids = torch.cat((torch.tensor([cls]), token_ids)) # will embed the cls as the prefix\n",
    "        batch.append(token_ids)\n",
    "    return torch.stack(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate, drop_last=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  3,  98, 336,  ...,  31, 288, 549],\n",
       "         [  8, 154, 415,  ...,  95, 346, 589],\n",
       "         [  9, 178, 464,  ..., 204, 452, 683],\n",
       "         ...,\n",
       "         [  8, 263, 521,  ...,  69, 358, 621],\n",
       "         [  5, 197, 461,  ..., 137, 401, 661],\n",
       "         [  0, 154, 420,  ..., 140, 397, 583]]),\n",
       " tensor([[  3, 168, 378,  ...,  31, 333, 632],\n",
       "         [  8, 245, 501,  ..., 196, 466, 721],\n",
       "         [  8, 168, 456,  ...,  17, 274, 529],\n",
       "         ...,\n",
       "         [  6,  98, 395,  ...,  56, 320, 540],\n",
       "         [  6,  67, 296,  ..., 125, 366, 591],\n",
       "         [  5, 137, 396,  ..., 113, 372, 585]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dl)), next(iter(val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(self, *, weight_decay, lr: float, betas: tuple[float, float]):\n",
    "    # start with all of the candidate parameters\n",
    "    param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "    # filter out those that do not require grad\n",
    "    param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "    # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "    # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "    optim_groups = [\n",
    "        {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "        {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    num_decay_params = sum(p.numel() for p in decay_params)\n",
    "    num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "    print(\n",
    "        f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\"\n",
    "    )\n",
    "    print(\n",
    "        f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\"\n",
    "    )\n",
    "    # Create AdamW optimizer and use the fused version if it is available\n",
    "    optimizer = torch.optim.AdamW(optim_groups, lr=lr, betas=betas, fused=True)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 73, with 20,701,184 parameters\n",
      "num non-decayed parameter tensors: 49, with 43,520 parameters\n"
     ]
    }
   ],
   "source": [
    "vocab_size = n_classes + C * 256\n",
    "\n",
    "lr = 3e-4\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "mamba_config = MambaConfig(\n",
    "    d_model=512,\n",
    "    n_layer=12,\n",
    "    # d_model=768,\n",
    "    # n_layer=24,\n",
    "    vocab_size=vocab_size,\n",
    "    ssm_cfg={},\n",
    "    rms_norm=True,\n",
    "    residual_in_fp32=True,\n",
    "    fused_add_norm=True,\n",
    "    pad_vocab_size_multiple=8,\n",
    ")\n",
    "\n",
    "\n",
    "model = MambaLMHeadModel(mamba_config).to(device)\n",
    "optimizer = configure_optimizers(model, weight_decay=0.01, lr=lr, betas=(0.9, 0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  3, 168, 378,  ...,  31, 333, 632],\n",
       "        [  8, 245, 501,  ..., 196, 466, 721],\n",
       "        [  8, 168, 456,  ...,  17, 274, 529],\n",
       "        ...,\n",
       "        [  6,  98, 395,  ...,  56, 320, 540],\n",
       "        [  6,  67, 296,  ..., 125, 366, 591],\n",
       "        [  5, 137, 396,  ..., 113, 372, 585]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.744704M parameters\n",
      "step=10 loss=5.828 grad_norm=0.919 throughput=39.60 samples/s\n",
      "step=20 loss=4.912 grad_norm=0.737 throughput=88.10 samples/s\n",
      "step=30 loss=4.298 grad_norm=0.834 throughput=88.12 samples/s\n",
      "step=40 loss=3.986 grad_norm=0.659 throughput=88.10 samples/s\n",
      "step=50 loss=3.847 grad_norm=1.313 throughput=88.10 samples/s\n",
      "step=60 loss=3.672 grad_norm=1.880 throughput=88.12 samples/s\n",
      "step=70 loss=3.627 grad_norm=1.356 throughput=88.11 samples/s\n",
      "step=80 loss=3.652 grad_norm=1.181 throughput=88.11 samples/s\n",
      "step=90 loss=3.439 grad_norm=1.234 throughput=88.07 samples/s\n",
      "step=100 loss=3.452 grad_norm=1.621 throughput=88.00 samples/s\n",
      "step=110 loss=3.458 grad_norm=1.858 throughput=88.00 samples/s\n",
      "step=120 loss=3.480 grad_norm=1.736 throughput=88.01 samples/s\n",
      "step=130 loss=3.333 grad_norm=1.147 throughput=88.02 samples/s\n",
      "step=140 loss=3.448 grad_norm=1.461 throughput=88.03 samples/s\n",
      "step=150 loss=3.304 grad_norm=0.989 throughput=88.01 samples/s\n",
      "step=160 loss=3.274 grad_norm=1.786 throughput=88.03 samples/s\n",
      "step=170 loss=3.361 grad_norm=1.577 throughput=88.02 samples/s\n",
      "step=180 loss=3.239 grad_norm=1.232 throughput=88.02 samples/s\n",
      "step=190 loss=3.414 grad_norm=1.307 throughput=88.02 samples/s\n",
      "step=200 loss=3.224 grad_norm=1.328 throughput=88.03 samples/s\n",
      "step=210 loss=3.326 grad_norm=1.301 throughput=88.01 samples/s\n",
      "step=220 loss=3.202 grad_norm=1.518 throughput=87.98 samples/s\n",
      "step=230 loss=3.170 grad_norm=1.512 throughput=87.79 samples/s\n",
      "step=240 loss=3.335 grad_norm=1.142 throughput=87.82 samples/s\n",
      "step=250 loss=3.242 grad_norm=1.351 throughput=87.81 samples/s\n",
      "step=260 loss=3.244 grad_norm=1.415 throughput=87.82 samples/s\n",
      "step=270 loss=3.124 grad_norm=1.184 throughput=87.80 samples/s\n",
      "step=280 loss=3.236 grad_norm=1.691 throughput=87.81 samples/s\n",
      "step=290 loss=3.225 grad_norm=1.392 throughput=87.81 samples/s\n",
      "step=300 loss=3.217 grad_norm=1.326 throughput=87.82 samples/s\n",
      "step=310 loss=3.206 grad_norm=1.645 throughput=87.81 samples/s\n",
      "step=320 loss=3.217 grad_norm=1.017 throughput=87.82 samples/s\n",
      "step=330 loss=3.133 grad_norm=1.322 throughput=87.81 samples/s\n",
      "step=340 loss=3.153 grad_norm=1.583 throughput=87.82 samples/s\n",
      "step=350 loss=3.194 grad_norm=1.496 throughput=87.81 samples/s\n",
      "step=360 loss=3.107 grad_norm=1.292 throughput=87.81 samples/s\n",
      "step=370 loss=3.130 grad_norm=0.992 throughput=87.81 samples/s\n",
      "step=380 loss=3.189 grad_norm=1.275 throughput=87.81 samples/s\n",
      "step=390 loss=3.058 grad_norm=1.488 throughput=87.80 samples/s\n",
      "step=400 loss=3.109 grad_norm=1.279 throughput=87.81 samples/s\n",
      "step=410 loss=3.116 grad_norm=1.179 throughput=87.82 samples/s\n",
      "step=420 loss=3.148 grad_norm=1.713 throughput=87.83 samples/s\n",
      "step=430 loss=3.065 grad_norm=1.159 throughput=87.80 samples/s\n",
      "step=440 loss=3.100 grad_norm=1.467 throughput=87.79 samples/s\n",
      "step=450 loss=3.143 grad_norm=1.279 throughput=87.80 samples/s\n",
      "step=460 loss=3.071 grad_norm=0.966 throughput=87.81 samples/s\n",
      "step=470 loss=3.152 grad_norm=1.063 throughput=87.80 samples/s\n",
      "step=480 loss=3.162 grad_norm=1.040 throughput=87.79 samples/s\n",
      "step=490 loss=3.036 grad_norm=1.291 throughput=87.80 samples/s\n",
      "step=500 loss=3.105 grad_norm=1.012 throughput=87.80 samples/s\n",
      "step=510 loss=3.137 grad_norm=1.031 throughput=87.80 samples/s\n",
      "step=520 loss=3.057 grad_norm=1.199 throughput=87.80 samples/s\n",
      "step=530 loss=3.039 grad_norm=1.351 throughput=87.80 samples/s\n",
      "step=540 loss=3.054 grad_norm=0.910 throughput=87.80 samples/s\n",
      "step=550 loss=3.262 grad_norm=1.246 throughput=87.78 samples/s\n",
      "step=560 loss=3.121 grad_norm=1.176 throughput=87.80 samples/s\n",
      "step=570 loss=3.100 grad_norm=1.014 throughput=87.81 samples/s\n",
      "step=580 loss=2.908 grad_norm=1.140 throughput=87.77 samples/s\n",
      "step=590 loss=3.118 grad_norm=1.163 throughput=87.79 samples/s\n",
      "step=600 loss=3.173 grad_norm=1.123 throughput=87.79 samples/s\n",
      "step=610 loss=3.048 grad_norm=0.863 throughput=87.79 samples/s\n",
      "step=620 loss=3.084 grad_norm=1.130 throughput=87.79 samples/s\n",
      "step=630 loss=2.940 grad_norm=0.977 throughput=87.81 samples/s\n",
      "step=640 loss=2.960 grad_norm=1.014 throughput=87.80 samples/s\n",
      "step=650 loss=3.058 grad_norm=1.101 throughput=87.81 samples/s\n",
      "step=660 loss=3.118 grad_norm=1.031 throughput=87.79 samples/s\n",
      "step=670 loss=3.084 grad_norm=0.965 throughput=87.80 samples/s\n",
      "step=680 loss=2.941 grad_norm=0.897 throughput=87.79 samples/s\n",
      "step=690 loss=3.009 grad_norm=0.860 throughput=87.81 samples/s\n",
      "step=700 loss=2.992 grad_norm=1.082 throughput=87.80 samples/s\n",
      "step=710 loss=2.959 grad_norm=1.176 throughput=87.79 samples/s\n",
      "step=720 loss=2.956 grad_norm=1.489 throughput=87.78 samples/s\n",
      "step=730 loss=3.091 grad_norm=0.953 throughput=87.78 samples/s\n",
      "step=740 loss=3.064 grad_norm=0.969 throughput=87.79 samples/s\n",
      "step=750 loss=2.890 grad_norm=1.132 throughput=87.79 samples/s\n",
      "step=760 loss=3.039 grad_norm=1.071 throughput=87.78 samples/s\n",
      "step=770 loss=3.093 grad_norm=0.765 throughput=87.79 samples/s\n",
      "step=780 loss=2.900 grad_norm=1.196 throughput=87.79 samples/s\n",
      "step=790 loss=2.997 grad_norm=0.970 throughput=87.79 samples/s\n",
      "step=800 loss=3.061 grad_norm=0.879 throughput=87.79 samples/s\n",
      "step=810 loss=3.003 grad_norm=1.077 throughput=87.79 samples/s\n",
      "step=820 loss=3.024 grad_norm=1.201 throughput=87.79 samples/s\n",
      "step=830 loss=3.078 grad_norm=1.049 throughput=87.79 samples/s\n",
      "step=840 loss=3.003 grad_norm=0.906 throughput=87.80 samples/s\n",
      "step=850 loss=3.002 grad_norm=0.970 throughput=87.79 samples/s\n",
      "step=860 loss=3.016 grad_norm=0.891 throughput=87.79 samples/s\n",
      "step=870 loss=3.010 grad_norm=0.962 throughput=87.79 samples/s\n",
      "step=880 loss=3.067 grad_norm=0.835 throughput=87.80 samples/s\n",
      "step=890 loss=3.001 grad_norm=0.781 throughput=87.78 samples/s\n",
      "step=900 loss=3.090 grad_norm=0.850 throughput=87.80 samples/s\n",
      "step=910 loss=3.009 grad_norm=1.048 throughput=87.80 samples/s\n",
      "step=920 loss=3.047 grad_norm=0.837 throughput=87.80 samples/s\n",
      "step=930 loss=2.942 grad_norm=1.322 throughput=87.81 samples/s\n",
      "step=940 loss=2.929 grad_norm=0.996 throughput=87.80 samples/s\n",
      "step=950 loss=3.012 grad_norm=0.903 throughput=87.79 samples/s\n",
      "step=960 loss=2.803 grad_norm=1.262 throughput=87.79 samples/s\n",
      "step=970 loss=2.987 grad_norm=0.710 throughput=87.79 samples/s\n",
      "step=980 loss=3.014 grad_norm=1.015 throughput=87.79 samples/s\n",
      "step=990 loss=3.071 grad_norm=0.780 throughput=87.81 samples/s\n",
      "step=1000 loss=3.067 grad_norm=0.877 throughput=87.80 samples/s\n",
      "step=1010 loss=2.963 grad_norm=1.049 throughput=87.80 samples/s\n",
      "step=1020 loss=2.910 grad_norm=0.909 throughput=87.80 samples/s\n",
      "step=1030 loss=2.982 grad_norm=0.790 throughput=87.79 samples/s\n",
      "step=1040 loss=3.042 grad_norm=0.757 throughput=87.80 samples/s\n",
      "step=1050 loss=2.835 grad_norm=0.838 throughput=87.79 samples/s\n",
      "step=1060 loss=3.063 grad_norm=0.914 throughput=87.79 samples/s\n",
      "step=1070 loss=2.897 grad_norm=0.806 throughput=87.80 samples/s\n",
      "step=1080 loss=3.022 grad_norm=0.700 throughput=87.80 samples/s\n",
      "step=1090 loss=2.972 grad_norm=0.751 throughput=87.81 samples/s\n",
      "step=1100 loss=2.970 grad_norm=1.052 throughput=87.79 samples/s\n",
      "step=1110 loss=2.913 grad_norm=0.689 throughput=87.79 samples/s\n",
      "step=1120 loss=2.999 grad_norm=0.830 throughput=87.80 samples/s\n",
      "step=1130 loss=2.979 grad_norm=0.673 throughput=87.80 samples/s\n",
      "step=1140 loss=2.842 grad_norm=0.953 throughput=87.80 samples/s\n",
      "step=1150 loss=2.826 grad_norm=0.952 throughput=87.81 samples/s\n",
      "step=1160 loss=2.892 grad_norm=1.254 throughput=87.79 samples/s\n",
      "step=1170 loss=2.805 grad_norm=0.923 throughput=87.81 samples/s\n",
      "step=1180 loss=2.939 grad_norm=1.035 throughput=87.80 samples/s\n",
      "step=1190 loss=2.882 grad_norm=0.697 throughput=87.81 samples/s\n",
      "step=1200 loss=3.034 grad_norm=0.675 throughput=87.80 samples/s\n",
      "step=1210 loss=2.878 grad_norm=0.827 throughput=87.79 samples/s\n",
      "step=1220 loss=2.824 grad_norm=0.942 throughput=87.78 samples/s\n",
      "step=1230 loss=2.866 grad_norm=0.730 throughput=87.78 samples/s\n",
      "step=1240 loss=2.912 grad_norm=0.741 throughput=87.80 samples/s\n",
      "step=1250 loss=2.836 grad_norm=0.863 throughput=87.79 samples/s\n",
      "step=1260 loss=2.899 grad_norm=0.842 throughput=87.80 samples/s\n",
      "step=1270 loss=2.992 grad_norm=1.095 throughput=87.79 samples/s\n",
      "step=1280 loss=2.931 grad_norm=0.807 throughput=87.80 samples/s\n",
      "step=1290 loss=2.808 grad_norm=0.888 throughput=87.80 samples/s\n",
      "step=1300 loss=2.859 grad_norm=0.705 throughput=87.81 samples/s\n",
      "step=1310 loss=3.012 grad_norm=0.863 throughput=87.80 samples/s\n",
      "step=1320 loss=2.897 grad_norm=1.120 throughput=87.80 samples/s\n",
      "step=1330 loss=2.824 grad_norm=0.907 throughput=87.78 samples/s\n",
      "step=1340 loss=2.756 grad_norm=0.981 throughput=87.74 samples/s\n",
      "step=1350 loss=2.928 grad_norm=0.952 throughput=87.79 samples/s\n",
      "step=1360 loss=2.963 grad_norm=0.708 throughput=87.78 samples/s\n",
      "step=1370 loss=2.924 grad_norm=0.705 throughput=87.79 samples/s\n",
      "step=1380 loss=2.864 grad_norm=0.878 throughput=87.80 samples/s\n",
      "step=1390 loss=2.737 grad_norm=0.739 throughput=87.78 samples/s\n",
      "step=1400 loss=2.970 grad_norm=0.769 throughput=87.80 samples/s\n",
      "step=1410 loss=2.875 grad_norm=0.697 throughput=87.78 samples/s\n",
      "step=1420 loss=2.845 grad_norm=0.724 throughput=87.79 samples/s\n",
      "step=1430 loss=2.871 grad_norm=0.690 throughput=87.78 samples/s\n",
      "step=1440 loss=2.857 grad_norm=0.861 throughput=87.80 samples/s\n",
      "step=1450 loss=2.885 grad_norm=1.106 throughput=87.78 samples/s\n",
      "step=1460 loss=2.794 grad_norm=1.040 throughput=87.78 samples/s\n",
      "step=1470 loss=2.907 grad_norm=0.802 throughput=87.78 samples/s\n",
      "step=1480 loss=2.972 grad_norm=0.954 throughput=87.79 samples/s\n",
      "step=1490 loss=2.812 grad_norm=0.918 throughput=87.77 samples/s\n",
      "step=1500 loss=2.930 grad_norm=0.758 throughput=87.79 samples/s\n",
      "step=1510 loss=2.920 grad_norm=0.750 throughput=87.79 samples/s\n",
      "step=1520 loss=2.846 grad_norm=0.615 throughput=87.79 samples/s\n",
      "step=1530 loss=2.857 grad_norm=0.887 throughput=87.79 samples/s\n",
      "step=1540 loss=2.938 grad_norm=0.673 throughput=87.80 samples/s\n",
      "step=1550 loss=2.954 grad_norm=0.861 throughput=87.77 samples/s\n",
      "step=1560 loss=3.024 grad_norm=1.017 throughput=87.79 samples/s\n",
      "End of epoch 1 val loss 2.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=1570 loss=2.983 grad_norm=0.693 throughput=8.79 samples/s\n",
      "step=1580 loss=2.896 grad_norm=0.927 throughput=87.84 samples/s\n",
      "step=1590 loss=2.804 grad_norm=0.845 throughput=87.85 samples/s\n",
      "step=1600 loss=2.897 grad_norm=0.903 throughput=87.84 samples/s\n",
      "step=1610 loss=3.093 grad_norm=0.787 throughput=87.84 samples/s\n",
      "step=1620 loss=2.918 grad_norm=0.848 throughput=87.83 samples/s\n",
      "step=1630 loss=2.874 grad_norm=0.810 throughput=87.84 samples/s\n",
      "step=1640 loss=2.850 grad_norm=0.946 throughput=87.84 samples/s\n",
      "step=1650 loss=2.887 grad_norm=0.734 throughput=87.84 samples/s\n",
      "step=1660 loss=2.897 grad_norm=0.877 throughput=87.83 samples/s\n",
      "step=1670 loss=2.936 grad_norm=0.710 throughput=87.85 samples/s\n",
      "step=1680 loss=2.836 grad_norm=0.743 throughput=87.85 samples/s\n",
      "step=1690 loss=2.748 grad_norm=0.792 throughput=87.84 samples/s\n",
      "step=1700 loss=2.793 grad_norm=0.710 throughput=87.84 samples/s\n",
      "step=1710 loss=2.909 grad_norm=0.706 throughput=87.86 samples/s\n",
      "step=1720 loss=2.802 grad_norm=0.734 throughput=87.84 samples/s\n",
      "step=1730 loss=2.834 grad_norm=0.669 throughput=87.84 samples/s\n",
      "step=1740 loss=3.001 grad_norm=1.057 throughput=87.86 samples/s\n",
      "step=1750 loss=2.855 grad_norm=0.873 throughput=87.84 samples/s\n",
      "step=1760 loss=2.931 grad_norm=0.743 throughput=87.83 samples/s\n",
      "step=1770 loss=3.003 grad_norm=0.740 throughput=87.82 samples/s\n",
      "step=1780 loss=2.929 grad_norm=1.046 throughput=87.85 samples/s\n",
      "step=1790 loss=2.728 grad_norm=0.767 throughput=87.85 samples/s\n",
      "step=1800 loss=2.940 grad_norm=0.643 throughput=87.85 samples/s\n",
      "step=1810 loss=2.823 grad_norm=0.884 throughput=87.84 samples/s\n",
      "step=1820 loss=2.891 grad_norm=0.753 throughput=87.85 samples/s\n",
      "step=1830 loss=2.625 grad_norm=0.934 throughput=87.84 samples/s\n",
      "step=1840 loss=2.932 grad_norm=0.755 throughput=87.85 samples/s\n",
      "step=1850 loss=2.745 grad_norm=0.830 throughput=87.84 samples/s\n",
      "step=1860 loss=2.906 grad_norm=0.822 throughput=87.85 samples/s\n",
      "step=1870 loss=2.860 grad_norm=0.769 throughput=87.85 samples/s\n",
      "step=1880 loss=2.943 grad_norm=0.634 throughput=87.85 samples/s\n",
      "step=1890 loss=2.991 grad_norm=0.740 throughput=87.86 samples/s\n",
      "step=1900 loss=2.746 grad_norm=0.879 throughput=87.85 samples/s\n",
      "step=1910 loss=2.971 grad_norm=0.735 throughput=87.85 samples/s\n",
      "step=1920 loss=2.829 grad_norm=0.966 throughput=87.84 samples/s\n",
      "step=1930 loss=2.902 grad_norm=0.671 throughput=87.85 samples/s\n",
      "step=1940 loss=2.772 grad_norm=0.659 throughput=87.85 samples/s\n",
      "step=1950 loss=2.895 grad_norm=0.796 throughput=87.84 samples/s\n",
      "step=1960 loss=2.885 grad_norm=0.764 throughput=87.84 samples/s\n",
      "step=1970 loss=2.802 grad_norm=0.774 throughput=87.84 samples/s\n",
      "step=1980 loss=2.779 grad_norm=0.892 throughput=87.85 samples/s\n",
      "step=1990 loss=2.747 grad_norm=0.719 throughput=87.84 samples/s\n",
      "step=2000 loss=2.763 grad_norm=0.783 throughput=87.84 samples/s\n",
      "step=2010 loss=2.885 grad_norm=0.680 throughput=87.83 samples/s\n",
      "step=2020 loss=2.680 grad_norm=1.127 throughput=87.85 samples/s\n",
      "step=2030 loss=2.708 grad_norm=1.191 throughput=87.85 samples/s\n",
      "step=2040 loss=2.913 grad_norm=0.607 throughput=87.85 samples/s\n",
      "step=2050 loss=2.938 grad_norm=0.804 throughput=87.82 samples/s\n",
      "step=2060 loss=2.751 grad_norm=0.957 throughput=87.84 samples/s\n",
      "step=2070 loss=2.802 grad_norm=0.972 throughput=87.83 samples/s\n",
      "step=2080 loss=2.776 grad_norm=0.657 throughput=87.83 samples/s\n",
      "step=2090 loss=2.708 grad_norm=1.102 throughput=87.84 samples/s\n",
      "step=2100 loss=2.912 grad_norm=0.819 throughput=87.84 samples/s\n",
      "step=2110 loss=2.880 grad_norm=0.636 throughput=87.83 samples/s\n",
      "step=2120 loss=2.798 grad_norm=0.770 throughput=87.85 samples/s\n",
      "step=2130 loss=2.659 grad_norm=0.746 throughput=87.84 samples/s\n",
      "step=2140 loss=2.823 grad_norm=0.911 throughput=87.85 samples/s\n",
      "step=2150 loss=2.648 grad_norm=1.158 throughput=87.86 samples/s\n",
      "step=2160 loss=2.786 grad_norm=0.781 throughput=87.84 samples/s\n",
      "step=2170 loss=2.810 grad_norm=0.747 throughput=87.85 samples/s\n",
      "step=2180 loss=2.900 grad_norm=0.786 throughput=87.83 samples/s\n",
      "step=2190 loss=2.797 grad_norm=0.778 throughput=87.84 samples/s\n",
      "step=2200 loss=2.789 grad_norm=0.635 throughput=87.85 samples/s\n",
      "step=2210 loss=2.727 grad_norm=0.889 throughput=87.84 samples/s\n",
      "step=2220 loss=2.723 grad_norm=0.854 throughput=87.85 samples/s\n",
      "step=2230 loss=2.877 grad_norm=0.700 throughput=87.84 samples/s\n",
      "step=2240 loss=2.739 grad_norm=0.850 throughput=87.85 samples/s\n",
      "step=2250 loss=2.858 grad_norm=0.726 throughput=87.83 samples/s\n",
      "step=2260 loss=2.782 grad_norm=0.950 throughput=87.85 samples/s\n",
      "step=2270 loss=2.800 grad_norm=0.655 throughput=87.84 samples/s\n",
      "step=2280 loss=2.847 grad_norm=0.602 throughput=87.84 samples/s\n",
      "step=2290 loss=2.900 grad_norm=0.678 throughput=87.83 samples/s\n",
      "step=2300 loss=2.934 grad_norm=0.920 throughput=87.85 samples/s\n",
      "step=2310 loss=2.854 grad_norm=0.692 throughput=87.84 samples/s\n",
      "step=2320 loss=2.838 grad_norm=0.732 throughput=87.85 samples/s\n",
      "step=2330 loss=2.838 grad_norm=0.729 throughput=87.83 samples/s\n",
      "step=2340 loss=2.849 grad_norm=0.661 throughput=87.84 samples/s\n",
      "step=2350 loss=2.682 grad_norm=0.822 throughput=87.85 samples/s\n",
      "step=2360 loss=2.812 grad_norm=0.836 throughput=87.84 samples/s\n",
      "step=2370 loss=2.684 grad_norm=0.927 throughput=87.84 samples/s\n",
      "step=2380 loss=2.923 grad_norm=0.675 throughput=87.85 samples/s\n",
      "step=2390 loss=2.840 grad_norm=0.735 throughput=87.85 samples/s\n",
      "step=2400 loss=2.875 grad_norm=0.718 throughput=87.83 samples/s\n",
      "step=2410 loss=2.831 grad_norm=0.830 throughput=87.84 samples/s\n",
      "step=2420 loss=2.737 grad_norm=0.674 throughput=87.83 samples/s\n",
      "step=2430 loss=2.821 grad_norm=1.131 throughput=87.85 samples/s\n",
      "step=2440 loss=2.631 grad_norm=0.979 throughput=87.84 samples/s\n",
      "step=2450 loss=2.825 grad_norm=1.091 throughput=87.84 samples/s\n",
      "step=2460 loss=2.757 grad_norm=0.709 throughput=87.82 samples/s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "log_every = 10\n",
    "max_norm = 1.0\n",
    "\n",
    "amp_dtype = (\n",
    "    torch.bfloat16\n",
    ")  # change to torch.float16 if your GPU doesn't support bfloat16\n",
    "\n",
    "assert torch.cuda.is_bf16_supported()\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=amp_dtype == torch.float16)\n",
    "n_parameters = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "print(f\"{n_parameters}M parameters\")\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "step = 0\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    for token_ids in train_dl:\n",
    "        token_ids = token_ids.to(device, non_blocking=True)\n",
    "        input_ids = token_ids[:, :-1].contiguous()\n",
    "        target_ids = token_ids[:, 1:].contiguous()\n",
    "\n",
    "        with torch.amp.autocast(dtype=amp_dtype, enabled=True, device_type=\"cuda\"):\n",
    "            output = model(input_ids)\n",
    "\n",
    "            logits = output.logits\n",
    "\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                target_ids.view(-1),\n",
    "            )\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.unscale_(optimizer)\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            t2 = time.perf_counter()\n",
    "            throughput = log_every * batch_size / (t2 - t1)\n",
    "            print(\n",
    "                f\"{step=} loss={loss.item():.3f} grad_norm={grad_norm.item():.3f} {throughput=:.2f} samples/s\"\n",
    "            )\n",
    "            t1 = t2\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for token_ids in val_dl:\n",
    "            token_ids = token_ids.to(device, non_blocking=True)\n",
    "            input_ids = token_ids[:, :-1].contiguous()\n",
    "            target_ids = token_ids[:, 1:].contiguous()\n",
    "\n",
    "            with torch.amp.autocast(dtype=amp_dtype, enabled=True, device_type=\"cuda\"):\n",
    "                output = model(input_ids)\n",
    "\n",
    "                logits = output.logits\n",
    "\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.view(-1, logits.size(-1)),\n",
    "                    target_ids.view(-1),\n",
    "                )\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "    val_loss = val_loss / len(val_dl)\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"End of epoch {epoch} val loss {val_loss:.3f}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    n = 6\n",
    "\n",
    "    temperature = 1.0\n",
    "    top_k = 64\n",
    "\n",
    "    generation_batch_size = n * n\n",
    "\n",
    "    # TODO doesn't like autocast here?\n",
    "    with torch.inference_mode():\n",
    "        input_ids = torch.randint(\n",
    "            0,\n",
    "            n_classes,\n",
    "            (\n",
    "                generation_batch_size,\n",
    "                1,\n",
    "            ),\n",
    "            device=device,\n",
    "        )\n",
    "        out = model.generate(\n",
    "            input_ids,\n",
    "            max_length=1 + H * W * C,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            cg=True,\n",
    "        )\n",
    "\n",
    "    generated_ids = out[:, 1:]\n",
    "    generated = rearrange(generated_ids, \"B (H W C) -> B H W C\", H=H, W=W, C=C)\n",
    "    generated = generated.cpu()\n",
    "    generated = generated - rgb_offsets - n_classes\n",
    "\n",
    "    checkpoint_name = f\"mamba-cifar-10-{int(n_parameters)}M-{epoch=}\"\n",
    "\n",
    "    fig, ax = plt.subplots(n, n, figsize=(16, 16))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax[i, j].imshow(generated[i * n + j])\n",
    "            ax[i, j].axis(\"off\")\n",
    "            class_name = class_names[input_ids[i * n + j].item()]\n",
    "            ax[i, j].set_title(class_name)\n",
    "\n",
    "    fig.savefig(f\"{checkpoint_name}.png\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{checkpoint_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
